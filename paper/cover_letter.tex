\documentclass[11pt,a4paper]{article}

\usepackage[margin=0.8in,top=0.6in,bottom=0.7in]{geometry}
\usepackage{hyperref}
\usepackage{setspace}
\singlespacing
\hypersetup{colorlinks=true,urlcolor=blue}

\pagestyle{empty}
\setlength{\parskip}{5pt}
\setlength{\parindent}{0pt}

\begin{document}

\begin{flushright}
\today
\end{flushright}

\vspace{-0.4cm}

Editor-in-Chief\\
\textit{AERA Open}\\
American Educational Research Association

\vspace{0.1cm}

Dear Editor,

We are pleased to submit our manuscript, ``When Algorithms Mirror Inequality: Structural Encoding of Fairness Disparities in Early Childhood Risk Prediction,'' for consideration as a regular article in \textit{AERA Open}.

This paper began with a straightforward question: if a school district deploys a machine learning tool to identify at-risk students, will it serve every child equally? The short answer is no---but not for the reasons most people assume.

Using the nationally representative ECLS-K:2011 ($N = 9{,}104$), we evaluated seven algorithms spanning logistic regression to modern gradient boosting. All seven produced nearly identical disparities---Hispanic students flagged at 2.4 times the rate of White students, Black students experiencing nearly four times higher calibration error. That convergence is the central finding: these gaps are not artifacts of any particular model. They are embedded in the data before any algorithm touches it.

Through counterfactual decomposition, we traced the pattern to its source. Statistically aligning Hispanic and White students on baseline characteristics did not merely close the gap---it reversed it. The portion attributable to differences in early cognitive scores and socioeconomic conditions exceeded the observed disparity by 52\%, revealing that algorithms were quietly \textit{compressing} inequality rather than amplifying it. Removing SES variables changed almost nothing, because kindergarten test scores already encode the same socioeconomic signal. In effect, these models operate as poverty detectors---and no algorithmic refinement can remedy that from within.

These findings speak directly to \textit{AERA Open}'s readership. First, they reframe the debate from ``which algorithm is fairest?'' to ``what institutional conditions produce data from which fair prediction is even possible?''---a question for educators and policymakers, not just computer scientists. Second, the paper provides a replicable template---combining calibration analysis, intersectional auditing, temporal stability testing, and uncertainty quantification---that researchers can apply to predictive tools increasingly common in K--12 settings.

The manuscript is approximately 10,000 words with 6 figures and 14 tables, plus Supplementary Materials with 9 figures and 8 tables. All analyses use freely available public-use data and reproducible open-source code. This manuscript has not been published or submitted elsewhere, and all authors have approved the submitted version.

We appreciate your time and consideration.

\vspace{0.2cm}

Sincerely,

Kismat Raj Vishwakarma\\
\textit{On behalf of all authors}

\vspace{0.5cm}

\noindent\rule{0.3\textwidth}{0.4pt}

\smallskip

\textbf{Corresponding Author}\\
Kismat Raj Vishwakarma\\
National Institute of Electronics \& Information Technology (NIELIT), India\\
Email: \href{mailto:kismat.vishwakarma@gmail.com}{kismat.vishwakarma@gmail.com}

\end{document}
