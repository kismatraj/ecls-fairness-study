\documentclass[12pt,a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{setspace}
\onehalfspacing
\hypersetup{colorlinks=true,urlcolor=blue}

\pagestyle{empty}

\begin{document}

\noindent \today

\bigskip

\noindent Editor-in-Chief\\
\textit{AERA Open}\\
American Educational Research Association

\bigskip

\noindent Dear Editor,

\medskip

We would like to submit our manuscript, ``When Algorithms Mirror Inequality: Structural Encoding of Fairness Disparities in Early Childhood Risk Prediction,'' for consideration as a regular article in \textit{AERA Open}.

This paper grew out of a simple question: if a school district adopts a machine learning tool to flag at-risk students, will it work equally well for every child? The answer, we found, is no---but the reasons are not what most people expect.

Using the nationally representative ECLS-K:2011 ($N = 9{,}104$), we tested seven algorithms ranging from logistic regression to modern gradient boosting. All seven produced essentially the same results. That convergence is itself the finding: the disparities we document---Hispanic students flagged at 2.4 times the rate of White students, Black students experiencing nearly four times higher calibration error---are not quirks of a particular model. They are baked into the data before any algorithm touches it.

We traced this pattern to its source through counterfactual decomposition. When we statistically aligned Hispanic and White students' baseline characteristics, the disparity didn't just disappear---it reversed. The gap attributable to differences in children's early cognitive scores and socioeconomic conditions actually exceeded the observed disparity by 52\%, meaning the algorithms were quietly \textit{compressing} inequality rather than amplifying it. Removing SES from the model changed almost nothing, because kindergarten test scores already carry the same socioeconomic signal. The model, in effect, works as a poverty detector---and no amount of algorithmic sophistication can fix that from inside the model.

We believe these findings matter for AERA Open's readership for two reasons. First, they shift the conversation from ``which algorithm is fairest?'' to ``what institutional conditions produce data from which fair prediction is possible?'' That is a question for educators and policymakers, not just computer scientists. Second, the paper offers a practical template---combining calibration analysis, intersectional auditing, temporal stability testing, and uncertainty quantification---that other researchers can apply to the predictive tools increasingly common in K--12 settings.

The manuscript is approximately 10,000 words with 6 figures and 14 tables, plus Supplementary Materials with 9 figures and 8 tables. All analyses use freely available public-use data and reproducible open-source code. This manuscript has not been published or submitted elsewhere, and all authors have approved the submitted version.

We appreciate your time and consideration.

\bigskip

\noindent Sincerely,

\medskip

\noindent Kismat Raj Vishwakarma (on behalf of all authors)

\bigskip

\noindent\textbf{Corresponding Author}\\
Kismat Raj Vishwakarma\\
National Institute of Electronics \& Information Technology (NIELIT), India\\
Email: \href{mailto:kismat.vishwakarma@gmail.com}{kismat.vishwakarma@gmail.com}

\end{document}
