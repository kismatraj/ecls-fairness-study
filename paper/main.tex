\documentclass[Afour,sageh,times,doublespace]{sagej}

% Additional packages (not loaded by sagej.cls)
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{array}
\usepackage{longtable}
\usepackage{xcolor}
\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

\emergencystretch=1em

\runninghead{Algorithmic Fairness in Early Childhood Risk Prediction}

\title{Algorithmic Fairness and Temporal Generalization in Early Childhood Risk Prediction: A Multi-Dimensional Audit Using the ECLS-K:2011 Longitudinal Study}

% Anonymized for double-blind review
\author{[Removed for double-blind review]}
\affiliation{[Removed for double-blind review]}

\begin{document}

\maketitle

\begin{abstract}
Machine learning models are increasingly used to identify at-risk students, yet fairness remains underexplored in longitudinal educational contexts. We conducted a comprehensive policy audit of seven algorithms predicting 5th-grade academic risk from early childhood data using the ECLS-K:2011 ($N = 9{,}104$; sensitivity analyses on the full sample of $N = 18{,}151$). All seven algorithms---including three state-of-the-art gradient boosting methods---converged on nearly identical performance (AUC range = 0.011), indicating that fairness failures are properties of the prediction task, not artifacts of model architecture. Bootstrap confidence intervals confirmed significant true positive rate disparities: Hispanic students were identified at 2.5 times the rate of White students. Black students experienced 3.35 times higher calibration error. Intersectional analysis suggested the model may function as a poverty detector, failing to identify at-risk high-SES Black students (TPR = 0\%, $N = 42$, $\approx$6 at-risk cases), though small cell sizes warrant cautious interpretation and replication. We identify a ``temporal fairness paradox'': additional longitudinal data improved accuracy but failed to resolve fairness disparities. Multiple imputation sensitivity analyses on the full sample confirmed that complete-case results are conservative---fairness disparities were larger when attrition-related bias was addressed. These findings argue for comprehensive, multi-dimensional fairness auditing before deploying algorithmic systems in education.
\end{abstract}

\begin{keywords}
Algorithmic fairness, machine learning, educational prediction, calibration fairness, intersectional fairness, temporal generalization, ECLS-K:2011
\end{keywords}

%==============================================================================
\section{Introduction}
%==============================================================================

The application of machine learning (ML) in educational settings has grown substantially over the past decade, with predictive models increasingly used to identify students at risk of academic failure, dropout, or other adverse outcomes \citep{baker2014educational}. These early warning systems (EWS) promise to enable timely interventions that could improve educational trajectories, particularly for disadvantaged students. However, the deployment of algorithmic decision-making tools in education raises critical questions about fairness and equity.

Algorithmic fairness---the study of how automated systems may systematically advantage or disadvantage particular groups---has emerged as a central concern in machine learning research \citep{mehrabi2021survey}. In educational contexts, unfair algorithms could perpetuate or amplify existing inequities by systematically under-identifying at-risk students from certain demographic groups or by disproportionately flagging students from marginalized communities for intervention. Recent work has highlighted the importance of examining not only group-level fairness metrics but also calibration equity, intersectional disparities, and the stability of fairness properties across analytical choices \citep{barocas2019fairness}.

This study addresses five primary research questions:

\begin{enumerate}
    \item \textbf{RQ1:} How accurately can early childhood cognitive and behavioral measures predict 5th-grade academic risk, and do state-of-the-art gradient boosting methods (LightGBM, CatBoost, HistGradientBoosting) outperform classical approaches?
    \item \textbf{RQ2:} Do predictive models exhibit differential performance across racial/ethnic and socioeconomic groups, as assessed by group-level metrics with bootstrap confidence intervals, calibration error, and intersectional (race $\times$ SES) analysis?
    \item \textbf{RQ3:} How do model predictions vary across demographic groups at the feature level, as revealed by SHAP-based explainability analysis?
    \item \textbf{RQ4:} Do fairness disparities change as models incorporate data from additional developmental time windows (temporal generalization)?
    \item \textbf{RQ5:} Can post-hoc bias mitigation strategies reduce fairness disparities, and how sensitive are fairness findings to the choice of at-risk threshold?
\end{enumerate}

We leverage the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), a nationally representative longitudinal study that followed children from kindergarten entry through 5th grade. This dataset provides a unique opportunity to examine both the predictive validity and fairness properties of models that use early childhood data to forecast later academic outcomes.

\subsection{Contributions}

Our goal is not to introduce novel algorithms but to conduct a rigorous \textit{policy audit} demonstrating that fairness failures are robust across seven algorithmic approaches. By showing that the source of inequity is structural rather than algorithmic, we shift the policy conversation from ``which model?'' to ``what data and what institutional practices produce these disparities?''

This study makes several contributions to the literature on algorithmic fairness in education:

\begin{itemize}
    \item We provide one of the first comprehensive, multi-dimensional fairness audits of longitudinal educational prediction models using nationally representative data, examining group fairness, calibration fairness, and intersectional fairness simultaneously.
    \item We demonstrate that the convergence of seven ML algorithms---including three state-of-the-art gradient boosting methods---on nearly identical performance (AUC range of 0.011) constitutes evidence that the source of unfairness is not algorithmic but structural, consistent with recent tabular ML benchmarks \citep{grinsztajn2022tree}.
    \item We employ SHAP-based explainability to examine whether predictive features operate differently across racial groups, providing transparency into the model's decision-making process \citep{lundberg2017unified}.
    \item We introduce calibration fairness and intersectional (race $\times$ SES) analysis to educational prediction, revealing suggestive patterns of under-identification of high-SES minority students that warrant replication with larger samples.\looseness=-1
    \item We identify a \textit{temporal fairness paradox}: additional longitudinal data from kindergarten through 3rd grade improves accuracy but fails to resolve---and in some cases worsens---fairness disparities. To our knowledge, this is the first empirical demonstration that more data does not naturally produce fairer educational predictions.
    \item We demonstrate the fragility of fairness assessments by showing that compliance with equal opportunity criteria depends critically on the choice of at-risk threshold.
    \item We conduct missing data sensitivity analyses (multiple imputation and inverse probability weighting) showing that complete-case results are conservative: fairness disparities are \textit{larger} in the full imputed sample, strengthening the primary conclusions.
\end{itemize}

%==============================================================================
\section{Background and Related Work}
%==============================================================================

\subsection{Early Warning Systems in Education}

Early warning systems (EWS) use student data to identify individuals at risk of negative academic outcomes. Traditional EWS relied on simple indicators such as attendance, behavior, and course performance (the ``ABC'' indicators). Modern approaches increasingly incorporate machine learning algorithms capable of processing larger feature sets and capturing nonlinear relationships \citep{lakkaraju2015machine}.

Research has demonstrated that ML-based EWS can achieve reasonable predictive accuracy, with AUC values typically ranging from 0.70 to 0.85 depending on the outcome and available features \citep{aguiar2015engagement}. Recent benchmarking studies have found that tree-based ensemble methods---including gradient boosting variants such as XGBoost, LightGBM \citep{ke2017lightgbm}, and CatBoost \citep{prokhorenkova2018catboost}---remain competitive with or superior to deep learning on structured tabular data \citep{grinsztajn2022tree}. However, fewer studies have examined whether these systems perform equitably across student subgroups.

\subsection{Algorithmic Fairness}

The machine learning fairness literature has developed numerous formal definitions of fairness, which can be broadly categorized into three families \citep{verma2018fairness}:

\textbf{Group fairness} criteria require that some statistical measure be equal across protected groups. Key definitions include:
\begin{itemize}
    \item \textit{Demographic parity} (statistical parity): The proportion of positive predictions should be equal across groups.
    \item \textit{Equal opportunity}: True positive rates should be equal across groups.
    \item \textit{Equalized odds}: Both true positive rates and false positive rates should be equal across groups.
\end{itemize}

\textbf{Individual fairness} requires that similar individuals receive similar predictions, regardless of group membership \citep{dwork2012fairness}.

\textbf{Counterfactual fairness} asks whether an individual's prediction would change if their protected attribute were different.

Importantly, researchers have proven that certain fairness criteria are mathematically incompatible, meaning it is generally impossible to satisfy all criteria simultaneously \citep{chouldechova2017fair, kleinberg2016inherent}.

\textbf{Calibration fairness} extends group fairness by examining whether predicted probabilities are equally reliable across groups. A model that is well-calibrated for one group but poorly calibrated for another may produce predictions that appear confident but are systematically misleading for certain populations \citep{pleiss2017calibration}. Expected Calibration Error (ECE) and Maximum Calibration Error (MCE) quantify the degree of miscalibration.

\textbf{Intersectional fairness} recognizes that examining single protected attributes in isolation may miss compounding disadvantages experienced by individuals at the intersection of multiple marginalized identities \citep{crenshaw1989demarginalizing, buolamwini2018gender}. Auditing fairness for subgroups defined by race $\times$ SES $\times$ gender can reveal disparities invisible to single-attribute analysis \citep{kearns2018preventing}.

\subsection{Explainability and Fairness}

Model interpretability plays a critical role in fairness auditing. SHAP (SHapley Additive exPlanations) values provide a unified framework for feature attribution, connecting game-theoretic concepts to local model explanations \citep{lundberg2017unified}. By computing SHAP values separately for each demographic group, analysts can detect whether the model relies on different features---or the same features with different magnitudes---when making predictions for different populations. Permutation importance with bootstrap confidence intervals provides a complementary, model-agnostic measure of feature relevance \citep{molnar2020interpretable}. When SHAP and permutation importance rankings agree, this strengthens confidence in the identified predictive mechanisms.

\subsection{Fairness in Educational AI}

A growing body of work has examined fairness in educational technology. \citet{kizilcec2022algorithmic} found that dropout prediction models in MOOCs exhibited significant performance disparities across countries. \citet{yu2020towards} demonstrated that automated essay scoring systems showed bias against non-native English speakers. \citet{gardner2019evaluating} examined fairness in course outcome prediction and found persistent gaps across demographic groups.

Despite this emerging literature, few studies have examined fairness in early childhood prediction contexts, in systems that make predictions across extended time horizons, or using the full suite of modern fairness metrics (calibration, intersectionality, uncertainty quantification). Our study addresses these gaps.

%==============================================================================
\section{Data and Methods}
%==============================================================================

\subsection{Data Source}

We used data from the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), conducted by the National Center for Education Statistics (NCES). The ECLS-K:2011 is a nationally representative longitudinal study that followed approximately 18,000 children from kindergarten entry in fall 2010 through spring of 5th grade in 2016.

Data were collected across nine waves:
\begin{itemize}
    \item Kindergarten: Fall 2010 (Wave 1), Spring 2011 (Wave 2)
    \item 1st Grade: Fall 2011 (Wave 3), Spring 2012 (Wave 4)
    \item 2nd Grade: Fall 2012 (Wave 5), Spring 2013 (Wave 6)
    \item 3rd Grade: Spring 2014 (Wave 7)
    \item 4th Grade: Spring 2015 (Wave 8)
    \item 5th Grade: Spring 2016 (Wave 9)
\end{itemize}

We used the public-use data file, which includes 18,174 children. After applying inclusion criteria (valid outcome data and at least some baseline predictors), our analytic sample comprised 18,151 children. Complete-case analysis for modeling yielded 9,104 children with data on all predictors and outcomes.

\subsection{Measures}

\subsubsection{Outcome Variable}

The primary outcome was \textbf{academic risk in 5th grade}, operationalized as scoring below the 25th percentile on the reading theta score (X9RTHETA) from the spring 2016 assessment. The reading assessment measured skills including basic reading, vocabulary, and reading comprehension. The theta score is an IRT-based ability estimate that allows for longitudinal comparisons. In our sample, 15.7\% of children were classified as at-risk based on this threshold.

As a secondary analysis, we also examined the math outcome (X9MTHETA) to assess domain-specificity of fairness findings.

\subsubsection{Predictor Variables}

We included predictors from kindergarten through 2nd grade across four domains:

\textbf{Baseline Cognitive Scores:}
\begin{itemize}
    \item Reading theta scores: Fall K (X1RTHETK), Spring K (X2RTHETK)
    \item Math theta scores: Fall K (X1MTHETK), Spring K (X2MTHETK)
\end{itemize}

\textbf{Executive Function:}
\begin{itemize}
    \item Dimensional Change Card Sort score, Spring 2013 (X6DCCSSCR)
\end{itemize}

\textbf{Approaches to Learning:}
\begin{itemize}
    \item Teacher-reported approaches to learning: Fall~K~(X1TCHAPP),\\ Spring~K~(X2TCHAPP), Spring 1st grade~(X4TCHAPP)
\end{itemize}

\textbf{Demographic Characteristics:}
\begin{itemize}
    \item Child sex (X\_CHSEX\_R)
    \item Race/ethnicity (X\_RACETH\_R)
    \item Socioeconomic status quintile (X1SESQ5)
    \item Home language (X12LANGST)
\end{itemize}

\subsubsection{Protected Attributes}

For fairness analysis, we focused on \textbf{race/ethnicity} as the primary protected attribute. The ECLS-K:2011 includes seven race/ethnicity categories; we collapsed these into five groups: White (reference), Black, Hispanic, Asian, and Other (including Native Hawaiian/Pacific Islander, American Indian/Alaska Native, and multiracial). For intersectional analysis, we crossed race/ethnicity with SES quintile.

\subsection{Machine Learning Models}

We trained seven classification algorithms spanning classical and state-of-the-art approaches:

\subsubsection{Classical Models}

\begin{enumerate}
    \item \textbf{Logistic Regression:} L2-regularized logistic regression with regularization strength selected via cross-validation from $C \in \{0.01, 0.1, 1.0, 10.0\}$.

    \item \textbf{Elastic Net:} Logistic regression with elastic net penalty, tuning both regularization strength $\alpha \in \{0.001, 0.01, 0.1, 1.0\}$ and L1~ratio~$\in \{0.2, 0.5, 0.8\}$.

    \item \textbf{Random Forest:} Ensemble of decision trees with hyperparameters: $n_\mathit{estimators} \in \{100, 200\}$, $max_\mathit{depth} \in \{5, 10, 15\}$, $min_\mathit{samples\_leaf} \in \{5, 10\}$.

    \item \textbf{XGBoost:} Gradient boosted trees \citep{chen2016xgboost} with $n_\mathit{estimators} \in \{100, 200\}$, $max_\mathit{depth} \in \{3, 5, 7\}$, $learning_\mathit{rate} \in \{0.01, 0.1\}$.
\end{enumerate}

\subsubsection{State-of-the-Art Gradient Boosting Methods}

\begin{enumerate}
    \setcounter{enumi}{4}
    \item \textbf{LightGBM:} Gradient boosting with leaf-wise tree growth and histogram-based binning \citep{ke2017lightgbm}. Hyperparameters: $n_\mathit{estimators} \in \{100, 200, 300\}$, $max_\mathit{depth} \in \{3, 5, 7, {-}1\}$, $learning_\mathit{rate} \in \{0.01, 0.05, 0.1\}$, $num_\mathit{leaves} \in \{31, 63, 127\}$.

    \item \textbf{CatBoost:} Gradient boosting with ordered boosting to reduce prediction shift and native handling of categorical features \citep{prokhorenkova2018catboost}. Hyperparameters: $iterations \in \{100, 200, 300\}$, $depth \in \{4, 6, 8\}$, $learning_\mathit{rate} \in \{0.01, 0.05, 0.1\}$.

    \item \textbf{HistGradientBoosting:} Scikit-learn's histogram-based gradient boosting, inspired by LightGBM, with native missing value support and early stopping. Hyperparameters: $max_\mathit{iter} \in \{100, 200, 300\}$, $max_\mathit{depth} \in \{3, 5, 7\}$, $learning_\mathit{rate} \in \{0.01, 0.05, 0.1\}$.
\end{enumerate}

Recent benchmarks suggest that tree-based ensemble methods remain competitive with or superior to deep learning on structured tabular data \citep{grinsztajn2022tree}. We include three recent gradient boosting variants to test whether state-of-the-art methods improve upon classical approaches for educational prediction.

All models were trained using 5-fold stratified cross-validation for hyperparameter selection, with random seed fixed at 42 for reproducibility. The data were split 70\% training, 30\% test.

\subsection{Evaluation Metrics}

\subsubsection{Predictive Performance}

We evaluated predictive performance using AUC-ROC, accuracy, precision (PPV), recall (sensitivity/TPR), F1 score, and Brier score.

\subsubsection{Fairness Metrics}

For each demographic group $g$, we computed:

\begin{itemize}
    \item \textbf{True Positive Rate (TPR):} $TPR_g = \frac{TP_g}{TP_g + FN_g}$
    \item \textbf{False Positive Rate (FPR):} $FPR_g = \frac{FP_g}{FP_g + TN_g}$
    \item \textbf{Positive Predictive Value (PPV):} $PPV_g = \frac{TP_g}{TP_g + FP_g}$
\end{itemize}

We assessed three fairness criteria:

\textbf{Equal Opportunity:} Satisfied if TPR ratios between groups exceed 0.80 (four-fifths rule).

\textbf{Equalized Odds:} Satisfied if both TPR and FPR ratios exceed 0.80.

\textbf{Statistical Parity:} Satisfied if positive rate ratios exceed 0.80.

\subsubsection{Bootstrap Confidence Intervals}

We computed 95\% bootstrap confidence intervals for all group-level fairness metrics using 500 bootstrap iterations, enabling formal statistical assessment of inter-group differences. Non-overlapping confidence intervals between groups indicate statistically significant disparities at approximately the $\alpha = 0.05$ level.

\subsubsection{Calibration Fairness}

We assessed calibration equity using Expected Calibration Error (ECE) and Maximum Calibration Error (MCE) computed separately for each demographic group. ECE measures the average absolute difference between predicted probabilities and observed frequencies across probability bins:

\begin{equation}
ECE = \sum_{b=1}^{B} \frac{n_b}{N} |acc(b) - conf(b)|
\end{equation}

where $acc(b)$ is the observed accuracy in bin $b$ and $conf(b)$ is the mean predicted probability. We computed ECE ratios relative to the White reference group to quantify differential calibration.

\subsubsection{Intersectional Fairness}

We examined fairness at the intersection of race/ethnicity and SES quintile, computing TPR, FPR, PPV, and accuracy for each race-by-SES subgroup with a minimum group size of 20. This analysis follows recommendations for rich subgroup fairness auditing \citep{kearns2018preventing} and intersectional analysis \citep{buolamwini2018gender}.

\subsection{Explainability Analysis}

We employed multiple explainability methods to understand predictive mechanisms:

\textbf{SHAP values:} We applied TreeExplainer \citep{lundberg2017unified} to the best-performing model. Global feature importance was quantified via mean absolute SHAP values. Local explanations revealed per-prediction feature contributions.

\textbf{Permutation importance:} We computed permutation importance with 50 bootstrap iterations to obtain 95\% confidence intervals for feature importance, providing a model-agnostic comparison to SHAP.

\textbf{Fairness-aware SHAP:} SHAP values were computed separately for each racial/ethnic group to detect whether predictive features operate with differential magnitude or direction across populations.\looseness=-1

\subsection{Temporal Generalization Analysis}

To examine how prediction timing affects both accuracy and fairness, we trained models under four temporal scenarios with progressively more features:

\begin{enumerate}
    \item \textbf{K Fall Only:} 7 features (fall kindergarten scores and demographics)
    \item \textbf{K Fall + Spring:} 10 features (adding spring kindergarten scores)
    \item \textbf{K + 1st Grade:} 11 features (adding 1st grade teacher report)
    \item \textbf{K through 3rd:} 12 features (adding executive function score)
\end{enumerate}

All seven algorithms were trained for each scenario. We examined how AUC, fairness metrics, and calibration error evolved across scenarios.

\subsection{Sensitivity Analysis}

We assessed sensitivity of fairness findings to the choice of at-risk threshold by repeating the full analysis at the 10th, 20th, 25th, and 30th percentiles. We re-evaluated all three fairness criteria at each threshold to determine whether fairness compliance was robust or threshold-dependent.

\subsection{Missing Data Sensitivity Analysis}

The ECLS-K:2011 uses special codes for missing data: $-1$ (not applicable), $-7$ (refused), $-8$ (don't know), and $-9$ (not ascertained). We recoded all such values to missing before analysis. Outcome attrition was substantial: 37\% of the initial sample lacked 5th-grade reading scores, and the complete-case analytic sample ($N = 9{,}104$) represented 50\% of the enrolled cohort. Missing data rates ranged from 5.3\% (spring kindergarten reading) to 37.0\% (5th-grade outcome), with executive function (24.1\%) and 1st-grade approaches to learning (25.9\%) showing intermediate rates. This level of attrition raises the possibility that our primary complete-case results are affected by selection bias if missingness is related to outcomes or protected attributes.

We conducted three complementary sensitivity analyses to assess the robustness of our findings to missing data:

\textbf{Attrition analysis.} We compared baseline characteristics of study completers ($N = 9{,}104$) and dropouts ($N = 9{,}047$) on all available baseline variables, using standardized mean differences (Cohen's $d$) for continuous variables and chi-squared tests for categorical variables \citep{rubin1987multiple}. We flagged differences exceeding $|d| \geq 0.20$ as potentially meaningful.

\textbf{Multiple Imputation by Chained Equations (MICE).} We generated $m = 10$ multiply-imputed datasets using iterative imputation with Bayesian ridge regression estimators and stochastic posterior draws, imputing the full sample ($N = 18{,}151$) rather than only complete cases. For each imputed dataset, we trained the elastic net model and computed group-level fairness metrics. Results were pooled using Rubin's rules: $\bar{Q} = \frac{1}{m}\sum_{i=1}^{m} Q_i$, with between-imputation variance $B = \frac{1}{m-1}\sum_{i=1}^{m}(Q_i - \bar{Q})^2$ and total variance $T = \bar{U} + (1 + 1/m)B$ \citep{rubin1987multiple}. Confidence intervals were computed as $\bar{Q} \pm 1.96\sqrt{T}$.

\textbf{Inverse Probability Weighting (IPW).} We estimated the probability of being a complete case using logistic regression on low-missingness baseline variables (race, sex, SES, kindergarten cognitive scores, home language). Inverse probability weights were computed as $w_i = 1/\hat{P}(\text{complete} \mid \mathbf{X}_i)$, stabilized by multiplying by the overall completion rate, and trimmed at the 99th percentile to limit the influence of extreme weights \citep{seaman2013review}. The elastic net model was then re-trained with these sample weights to assess whether reweighting to approximate the full-sample distribution altered fairness conclusions.

\subsection{Bias Mitigation}

We implemented \textbf{threshold optimization} as a post-processing bias mitigation strategy. Rather than using a single decision threshold (typically 0.5) for all groups, we selected group-specific thresholds to equalize true positive rates across groups. The target TPR was set to the overall TPR of the best-performing model.

We note that group-specific decision thresholds constitute a form of race-conscious classification, which raises legal and ethical concerns analogous to those surrounding affirmative action in other domains. Under the Equal Protection Clause, race-conscious government actions are subject to strict scrutiny, requiring a compelling interest and narrow tailoring \citep{barocas2019fairness}. We present threshold optimization as a \textit{diagnostic tool} to illustrate the theoretical bounds of post-hoc equalization, not as a recommended intervention for deployment.

%==============================================================================
\section{Results}
%==============================================================================

\subsection{Sample Characteristics}

Table \ref{tab:sample} presents the demographic characteristics of the analytic sample.

\begin{table*}[ht!]
\centering
\caption{Sample Characteristics (N = 18,151)}
\label{tab:sample}
\begin{tabular}{lrr}
\toprule
\textbf{Characteristic} & \textbf{N} & \textbf{\%} \\
\midrule
\multicolumn{3}{l}{\textit{Race/Ethnicity}} \\
\quad White & 8,476 & 46.7 \\
\quad Hispanic & 4,206 & 23.2 \\
\quad Black & 2,394 & 13.2 \\
\quad Other & 1,825 & 10.1 \\
\quad Asian & 380 & 2.1 \\
\quad Missing & 870 & 4.8 \\
\midrule
\multicolumn{3}{l}{\textit{SES Quintile}} \\
\quad Q1 (Lowest) & 3,224 & 17.8 \\
\quad Q2 & 3,214 & 17.7 \\
\quad Q3 & 3,217 & 17.7 \\
\quad Q4 & 3,227 & 17.8 \\
\quad Q5 (Highest) & 3,206 & 17.7 \\
\quad Missing & 2,063 & 11.4 \\
\midrule
\multicolumn{3}{l}{\textit{Sex}} \\
\quad Male & 9,273 & 51.1 \\
\quad Female & 8,840 & 48.7 \\
\midrule
\multicolumn{3}{l}{\textit{5th Grade Reading Risk}} \\
\quad At-Risk (<25th \%ile) & 2,857 & 15.7 \\
\quad Not At-Risk & 15,294 & 84.3 \\
\bottomrule
\end{tabular}
\end{table*}

The sample is demographically diverse, with substantial representation of historically underserved groups. Approximately 15.7\% of children were classified as at-risk in reading by 5th grade.

\subsection{Model Performance}

Table \ref{tab:performance} presents the predictive performance of all seven models on the held-out test set (N = 2,732).

\begin{table*}[ht!]
\centering
\caption{Model Performance on Test Set}
\label{tab:performance}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{AUC} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Brier} \\
\midrule
Elastic Net & \textbf{0.848} & \textbf{0.851} & 0.675 & 0.283 & 0.399 & \textbf{0.108} \\
Logistic Regression & 0.847 & 0.849 & 0.657 & 0.281 & 0.394 & 0.108 \\
CatBoost & 0.846 & 0.852 & 0.662 & \textbf{0.308} & \textbf{0.421} & 0.107 \\
Random Forest & 0.841 & 0.848 & 0.645 & 0.285 & 0.395 & 0.109 \\
XGBoost & 0.840 & 0.846 & 0.618 & 0.302 & 0.406 & 0.109 \\
Hist Gradient Boosting & 0.839 & 0.846 & 0.623 & 0.298 & 0.403 & 0.109 \\
LightGBM & 0.837 & 0.846 & 0.629 & 0.291 & 0.398 & 0.110 \\
\bottomrule
\end{tabular}
\end{table*}

All seven models achieved similar performance (Figure \ref{fig:model_comparison}), with AUC values ranging from 0.837 (LightGBM) to 0.848 (Elastic Net). The two classical regularized linear models (Elastic Net: 0.848; Logistic Regression: 0.847) achieved the highest discrimination, while the three state-of-the-art gradient boosting methods performed comparably but slightly lower (CatBoost: 0.846; HistGradientBoosting: 0.839; LightGBM: 0.837). CatBoost achieved the highest recall (0.308) and F1 score (0.421), making it the most effective at identifying at-risk students at the default threshold. The negligible AUC range across algorithms (0.011) suggests the performance ceiling is determined by the available features rather than algorithmic sophistication. The elastic net model was selected for subsequent fairness analysis based on its highest AUC.

\begin{figure*}[ht!]
\centering
\includegraphics[width=0.85\textwidth]{../results/figures/model_comparison.pdf}
\caption{Performance Comparison Across All Seven Models. AUC values ranged from 0.837 to 0.848, with classical regularized models matching or exceeding state-of-the-art gradient boosting methods.}
\label{fig:model_comparison}
\end{figure*}

\subsection{Explainability Analysis}

\subsubsection{Feature Importance}

Table \ref{tab:features} presents feature importance from three complementary methods: SHAP values, elastic net coefficients, and permutation importance with bootstrap confidence intervals.

\begin{table*}[ht!]
\centering
\caption{Feature Importance: SHAP, Elastic Net Coefficients, and Permutation Importance}
\label{tab:features}
\footnotesize
\begin{tabular}{lccc}
\toprule
\textbf{Feature} & \textbf{Mean $|$SHAP$|$} & \textbf{EN Coef.} & \textbf{Perm.\ Imp.\ [95\% CI]} \\
\midrule
Spring K Math (X2MTHETK) & 0.410 & 0.501 & 0.046 [0.042, 0.054] \\
Spring K Reading (X2RTHETK) & 0.254 & 0.350 & 0.023 [0.018, 0.027] \\
SES Quintile (X1SESQ5) & 0.253 & 0.303 & 0.013 [0.007, 0.019] \\
Fall K Math (X1MTHETK) & 0.227 & 0.288 & 0.012 [0.007, 0.017] \\
Approaches to Learning, 1st (X4TCHAPP) & 0.218 & 0.266 & 0.016 [0.012, 0.022] \\
Executive Function (X6DCCSSCR) & 0.067 & 0.115 & 0.001 [$-$0.001, 0.003] \\
Fall K Reading (X1RTHETK) & 0.000 & 0.038 & 0.000 \\
Child Sex, Race, Language, ATL (K) & 0.000 & 0.000 & 0.000 \\
\bottomrule
\end{tabular}
\end{table*}

Spring kindergarten math (X2MTHETK) was the dominant predictor across all methods (Figure \ref{fig:shap}), with mean $|$SHAP$|$ = 0.410, accounting for approximately 34\% of total SHAP importance. SHAP and permutation importance rankings showed high agreement (mean agreement = 0.87), with both methods identifying the same top-5 features. Notably, race/ethnicity, home language, child sex, and early approaches-to-learning measures received zero importance across all methods, confirming that elastic net regularization effectively excluded these features from the final model.

\begin{figure*}[ht!]
\centering
\includegraphics[width=0.85\textwidth]{../results/figures/shap_summary.pdf}
\caption{SHAP Summary Plot (Beeswarm). Each point represents one prediction. Color indicates feature value (red = high, blue = low). Spring K math score shows the widest spread, indicating it is the strongest predictor with clear directionality.}
\label{fig:shap}
\end{figure*}

\subsubsection{Fairness-Aware SHAP}

We computed SHAP values separately for each racial/ethnic group to detect differential feature importance. The top-5 feature rankings were identical across White, Black, and Hispanic subgroups. However, the magnitude of math score importance was somewhat higher for Black and Hispanic students compared to White students, suggesting that cognitive scores carry relatively more predictive weight for minority students. These magnitude differences were modest and did not alter the overall ranking of features, indicating that the model uses a consistent predictive mechanism across groups rather than relying on group-specific pathways.

\subsection{Fairness Analysis}

\subsubsection{Group-Level Performance with Confidence Intervals}

Table \ref{tab:fairness} presents performance metrics by racial/ethnic group with bootstrap 95\% confidence intervals.

\begin{table*}[ht!]
\centering
\caption{Model Performance by Race/Ethnicity with 95\% Bootstrap Confidence Intervals}
\label{tab:fairness}
\begin{tabular}{lrccc}
\toprule
\textbf{Group} & \textbf{N} & \textbf{TPR [95\% CI]} & \textbf{FPR [95\% CI]} & \textbf{PPV [95\% CI]} \\
\midrule
White & 1,462 & 0.160 [0.113, 0.206] & 0.011 [0.006, 0.017] & 0.660 [0.500, 0.794] \\
Hispanic & 623 & 0.393 [0.326, 0.459] & 0.063 [0.044, 0.087] & 0.722 [0.637, 0.798] \\
Black & 300 & 0.296 [0.207, 0.388] & 0.095 [0.052, 0.133] & 0.508 [0.341, 0.667] \\
Other & 225 & 0.258 [0.074, 0.445] & 0.005 [0.000, 0.015] & 0.871 [0.571, 1.000] \\
Asian & 8 & 0.760 [0.250, 1.000] & 0.000 [0.000, 0.000] & 1.000 [1.000, 1.000] \\
\bottomrule
\multicolumn{5}{l}{\footnotesize \textit{Note.} Asian $N = 8$ in the test set is too small for reliable inference;} \\
\multicolumn{5}{l}{\footnotesize confidence intervals for groups with $N < 30$ may not achieve nominal coverage.} \\
\end{tabular}
\end{table*}

Bootstrap confidence intervals enable formal statistical comparison of inter-group differences (Figure \ref{fig:tpr_ci}). The Hispanic TPR of 0.393 [0.326, 0.459] was significantly higher than the White TPR of 0.160 [0.113, 0.206], with non-overlapping confidence intervals ($p < 0.05$). The Black-White TPR difference (0.296 vs. 0.160) showed partially overlapping CIs, suggesting a marginally significant difference. The model detected at-risk Hispanic students at 2.5 times the rate of at-risk White students.

We emphasize that the Asian subgroup ($N = 8$, CI: 0.250--1.000) is too small for any meaningful inference; its extreme TPR of 0.760 and perfect PPV are essentially uninformative and should not be interpreted as evidence of model performance for Asian students. Similarly, the Other subgroup ($N = 225$, but only $\approx$27 at-risk) yields wide confidence intervals (TPR CI: 0.074--0.445) that render point estimates exploratory at best.

False positive rate disparities were also substantial: Black students experienced an FPR of 0.095 [0.052, 0.133], compared to 0.011 [0.006, 0.017] for White students---approximately 8.6 times higher. This means non-at-risk Black children were far more likely to be incorrectly flagged. ROC curves by group (Figure \ref{fig:roc}) and calibration curves (Figure \ref{fig:calibration}) further illustrate these differential performance patterns.

\begin{figure*}[ht!]
\centering
\includegraphics[width=0.85\textwidth]{../results/figures/fairness_tpr_with_ci.pdf}
\caption{True Positive Rate by Racial/Ethnic Group with 95\% Bootstrap Confidence Intervals. Non-overlapping intervals between Hispanic and White groups confirm statistically significant TPR disparities.}
\label{fig:tpr_ci}
\end{figure*}

\subsubsection{Disparity Analysis}

Table \ref{tab:disparity} presents formal disparity metrics comparing each group to the White reference group.

\begin{table*}[ht!]
\centering
\caption{Fairness Disparity Metrics (Reference: White)}
\label{tab:disparity}
\begin{tabular}{lccccc}
\toprule
\textbf{Group} & \textbf{TPR Ratio} & \textbf{TPR Diff} & \textbf{FPR Ratio} & \textbf{FPR Diff} & \textbf{Disp. Impact} \\
\midrule
Asian & 4.750 & +0.600 & 0.000 & $-$0.011 & No \\
Hispanic & 2.458 & +0.233 & 5.662 & +0.052 & No \\
Black & 1.851 & +0.136 & 8.494 & +0.084 & No \\
Other & 1.611 & +0.098 & 0.477 & $-$0.006 & No \\
\bottomrule
\end{tabular}
\end{table*}

\textbf{Fairness Criteria Assessment:}

\begin{itemize}
    \item \textbf{Equal Opportunity:} PASS. All groups had TPR ratios $>$ 0.80 (all minority groups had \textit{higher} TPR than White students).
    \item \textbf{Equalized Odds:} FAIL. While TPR ratios exceeded 0.80, FPR ratios for Black (8.494) and Hispanic (5.662) students dramatically exceeded 1.0, indicating disproportionately high false positive rates.
    \item \textbf{Statistical Parity:} PASS. Positive prediction rates did not trigger the 0.80 disparate impact threshold.
\end{itemize}

\subsubsection{Calibration Fairness}

Table \ref{tab:calibration} presents calibration metrics by demographic group.

\begin{table*}[ht!]
\centering
\caption{Calibration Error by Demographic Group}
\label{tab:calibration}
\begin{tabular}{lrcccc}
\toprule
\textbf{Group} & \textbf{N} & \textbf{ECE} & \textbf{MCE} & \textbf{Brier} & \textbf{ECE Ratio} \\
\midrule
White & 1,462 & 0.022 & 0.112 & 0.082 & 1.00 \\
Hispanic & 623 & 0.036 & 0.115 & 0.155 & 1.65 \\
Other & 225 & 0.051 & 0.940 & 0.090 & 2.31 \\
Black & 300 & 0.074 & 0.456 & 0.162 & \textbf{3.35} \\
\bottomrule
\end{tabular}
\end{table*}

Calibration fairness analysis revealed substantial disparities (Figure \ref{fig:calibration_error}). Black students experienced an ECE of 0.074, 3.35 times higher than White students (0.022), indicating that predicted risk probabilities for Black students were systematically miscalibrated. The Maximum Calibration Error for Black students (MCE = 0.456) was 4.1 times the White MCE (0.112), indicating severe miscalibration in certain probability ranges. These calibration disparities mean that even when the model makes the correct binary classification, the confidence levels are less reliable for minority students---a critical concern when practitioners use predicted probabilities to prioritize intervention resources.

\begin{figure*}[ht!]
\centering
\includegraphics[width=0.85\textwidth]{../results/figures/calibration_error_comparison.pdf}
\caption{Expected and Maximum Calibration Error by Demographic Group. Black students experience calibration error 3.35 times higher than the White reference group.}
\label{fig:calibration_error}
\end{figure*}

\subsubsection{Intersectional Fairness}

Table \ref{tab:intersectional} presents fairness metrics for selected race $\times$ SES subgroups, revealing patterns invisible in single-attribute analysis.

\begin{table*}[ht!]
\centering
\caption{Intersectional Fairness: Selected Race $\times$ SES Subgroups}
\label{tab:intersectional}
\begin{tabular}{lrcccc}
\toprule
\textbf{Subgroup} & \textbf{N} & \textbf{Prevalence} & \textbf{TPR} & \textbf{FPR} & \textbf{Accuracy} \\
\midrule
\multicolumn{6}{l}{\textit{Low SES (Q1)}} \\
Hispanic Q1 & 269 & 38.7\% & 0.481 & 0.121 & 0.725 \\
Black Q1 & 80 & 32.5\% & 0.423 & 0.204 & 0.675 \\
White Q1 & 107 & 28.0\% & 0.400 & 0.117 & 0.748 \\
\addlinespace
\multicolumn{6}{l}{\textit{Mid SES (Q2--Q3)}} \\
Hispanic Q2 & 145 & 29.0\% & 0.452 & 0.058 & 0.800 \\
Black Q3 & 60 & 26.7\% & 0.375 & 0.068 & 0.783 \\
White Q3 & 314 & 13.4\% & 0.167 & 0.004 & 0.885 \\
\addlinespace
\multicolumn{6}{l}{\textit{High SES (Q4)}} \\
White Q4 & 398 & 9.0\% & 0.083 & 0.003 & 0.915 \\
Hispanic Q4 & 62 & 16.1\% & 0.200 & 0.019 & 0.855 \\
Black Q4 & 42 & 14.3\% & \textbf{0.000} & 0.056 & 0.810 \\
\bottomrule
\end{tabular}
\end{table*}

A suggestive finding was the model's failure to identify any at-risk Black students in the 4th SES quintile (TPR = 0\%, $N = 42$, approximately 6 at-risk cases), as shown in Figure~\ref{fig:intersectional}. Despite a 14.3\% prevalence of academic risk in this subgroup, the model flagged none of these students. However, we urge caution in interpreting this result: with only $\approx$6 positive cases, a binomial test of TPR = 0 against a null of TPR = 0.283 (the overall model TPR) yields $p = 0.41$, which is not statistically significant. The observed TPR = 0\% is consistent with sampling variability in a small cell. This pattern of potential ``intersectional invisibility'' extended to other high-SES minority subgroups: Hispanic Q4 achieved only 20\% TPR ($N = 62$, $\approx$10 at-risk). In contrast, low-SES students across all racial groups had relatively higher TPRs (0.400 to 0.481), consistent with the model's reliance on SES as a predictor.

The intersectional pattern is consistent with a plausible working hypothesis that the model operates as a \textit{poverty detector}: it identifies at-risk students primarily through socioeconomic signals, missing those whose risk arises from other factors. This interpretation is supported by the strong SES gradient visible across all racial groups (Table~\ref{tab:intersectional}), but the small cell sizes in high-SES minority subgroups---particularly the 3--6 positive cases that drive the most extreme TPR estimates---fall below thresholds for reliable estimation and require replication with larger samples before informing policy conclusions.

\begin{figure*}[ht!]
\centering
\includegraphics[width=0.85\textwidth]{../results/figures/intersectional_fairness_heatmap.pdf}
\caption{Intersectional Fairness Heatmap: TPR by Race $\times$ SES Quintile. The model identifies no at-risk Black students in the 4th SES quintile (TPR = 0\%, $N = 42$, $\approx$6 at-risk), though this cell size precludes definitive conclusions ($p = 0.41$). Interpret subgroups with $N < 50$ with caution.}
\label{fig:intersectional}
\end{figure*}

\begin{figure*}[ht!]
\centering
\includegraphics[width=0.8\textwidth]{../results/figures/roc_curves_by_group.pdf}
\caption{ROC Curves by Racial/Ethnic Group}
\label{fig:roc}
\end{figure*}

\begin{figure*}[ht!]
\centering
\includegraphics[width=0.8\textwidth]{../results/figures/calibration_curves_by_group.pdf}
\caption{Calibration Curves by Racial/Ethnic Group. Deviation from the diagonal indicates miscalibration.}
\label{fig:calibration}
\end{figure*}

\subsection{Temporal Generalization}

Table \ref{tab:temporal} presents model performance across four temporal scenarios with progressively more features.

\begin{table*}[ht!]
\centering
\caption{Best Model Performance Across Temporal Scenarios}
\label{tab:temporal}
\begin{tabular}{llccccc}
\toprule
\textbf{Scenario} & \textbf{Best Model} & \textbf{Features} & \textbf{AUC} & \textbf{Accuracy} & \textbf{F1} & \textbf{Brier} \\
\midrule
K Fall Only & Logistic Reg. & 7 & 0.799 & 0.837 & 0.343 & 0.119 \\
K Fall + Spring & Logistic Reg. & 10 & 0.822 & 0.844 & 0.372 & 0.113 \\
K + 1st Grade & Logistic Reg. & 11 & 0.829 & 0.845 & 0.395 & 0.112 \\
K through 3rd & Logistic Reg. & 12 & 0.831 & 0.843 & 0.386 & 0.112 \\
\bottomrule
\end{tabular}
\end{table*}

Logistic regression was the best-performing model across all four temporal scenarios (Figure \ref{fig:temporal_performance}), reinforcing the finding that classical methods are sufficient for this prediction task. AUC improved from 0.799 (K fall only) to 0.831 (K through 3rd grade), a meaningful gain of 0.032 AUC points. However, returns diminished rapidly: the largest gain came from adding spring kindergarten scores (+0.023 AUC), while adding 1st through 3rd grade data contributed only +0.009.

Critically, while overall accuracy improved with more data, fairness disparities did not narrow proportionally (Figure \ref{fig:temporal_disparity}). The Hispanic-White TPR gap remained substantial across all scenarios, and calibration disparities persisted. We term this the \textit{temporal fairness paradox}: additional longitudinal data improves accuracy without resolving---and in some cases worsening---fairness disparities. To our knowledge, this is the first empirical demonstration that the widely held assumption that ``more data produces fairer predictions'' does not hold in longitudinal educational settings. The paradox arises because additional developmental data reinforces the same socioeconomic signals that drive differential performance, rather than introducing orthogonal information that could equalize predictions across groups.

\begin{figure*}[ht!]
\centering
\includegraphics[width=0.85\textwidth]{../results/figures/temporal_performance_trend.pdf}
\caption{Model Performance Across Temporal Scenarios. AUC improves from 0.799 (K fall only) to 0.831 (K through 3rd grade), with diminishing returns after spring kindergarten.}
\label{fig:temporal_performance}
\end{figure*}

\begin{figure*}[ht!]
\centering
\includegraphics[width=0.85\textwidth]{../results/figures/temporal_disparity_heatmap.pdf}
\caption{Temporal Disparity Heatmap: TPR Ratios Across Developmental Windows. Fairness disparities persist across all temporal scenarios, indicating that additional data does not resolve equity concerns.}
\label{fig:temporal_disparity}
\end{figure*}

\subsection{Sensitivity Analysis}

Table \ref{tab:sensitivity} presents fairness criteria compliance across four at-risk threshold definitions.

\begin{table*}[ht!]
\centering
\caption{Fairness Criteria by At-Risk Threshold Definition}
\label{tab:sensitivity}
\begin{tabular}{lccccc}
\toprule
\textbf{Percentile} & \textbf{Prevalence} & \textbf{Equal Opp.} & \textbf{Equalized Odds} & \textbf{Statistical Parity} \\
\midrule
10th & 6.3\% & FAIL & FAIL & FAIL \\
20th & 12.6\% & FAIL & FAIL & FAIL \\
25th & 15.7\% & PASS & FAIL & PASS \\
30th & 18.9\% & FAIL & FAIL & FAIL \\
\bottomrule
\end{tabular}
\end{table*}

Sensitivity analysis revealed that fairness findings were highly dependent on the threshold definition. The 25th percentile was the only threshold at which the model passed equal opportunity and statistical parity criteria. At all other thresholds---including the nearby 20th and 30th percentiles---the model failed all three fairness criteria. This fragility underscores that fairness assessments cannot be divorced from the operationalization of the outcome, and that claims of fairness compliance are contingent on specific analytical choices.

\subsection{Missing Data Sensitivity Results}

\subsubsection{Attrition Analysis}

Study completers differed systematically from dropouts on all baseline cognitive variables. Completers scored higher on fall kindergarten reading ($d = 0.22$), spring kindergarten reading ($d = 0.26$), fall kindergarten math ($d = 0.28$), and spring kindergarten math ($d = 0.30$). Attrition was also differential by demographics: White students were over-represented among completers (53.4\% vs.\ 40.0\% of dropouts), while Asian (0.4\% vs.\ 3.9\%), Black (11.0\% vs.\ 15.4\%), and Other (7.8\% vs.\ 12.3\%) students were under-represented. These patterns indicate that complete-case analysis likely under-represents the most disadvantaged students, motivating the MICE and IPW sensitivity analyses.

\subsubsection{Multiple Imputation Results}

MICE on the full sample ($N = 18{,}151$, $m = 10$ imputations) yielded slightly higher overall AUC (0.864, SE = 0.006) compared to complete-case analysis (0.848). More importantly, the imputed-sample analysis revealed \textit{larger} fairness disparities than the complete-case analysis. Black TPR increased from 0.293 to 0.530 (difference = +0.237); Hispanic TPR from 0.410 to 0.552 (+0.142); White TPR from 0.172 to 0.374 (+0.202). The Hispanic-White TPR ratio remained elevated (1.48 in the imputed sample vs.\ 2.46 in complete-case), and the Black-White ratio increased (1.42 vs.\ 1.85). These results confirm that the complete-case analysis is conservative: the attrition-corrected estimates show that fairness disparities persist and may be larger in the full population.

\subsubsection{IPW Results}

Inverse probability weights were well-behaved: mean = 0.77, SD = 0.08, range 0.58--1.03, with 92 observations trimmed at the 99th percentile. The IPW-weighted analysis produced virtually identical results to the unweighted analysis (AUC = 0.848 in both cases). Group-level metrics showed minimal changes: the largest FPR shift was $-$0.009 for Black and Hispanic students. This convergence between IPW and unweighted results provides additional evidence that the fairness findings are robust to selection bias adjustment.

\subsection{Outcome Comparison: Reading vs. Math}

As a secondary analysis, we compared reading and math outcomes. Math prediction (AUC = 0.867, best model: Logistic Regression) substantially outperformed reading prediction (AUC = 0.848, best model: Elastic Net). Fairness patterns were broadly similar across domains, with Hispanic and Black students showing higher TPR than White students for both outcomes. However, math prediction uniquely exhibited disparate impact for the Other racial group (TPR ratio = 0.24 vs. White), a pattern not observed in reading. This suggests that the domain of the outcome can affect which groups experience adverse fairness impacts, reinforcing the importance of outcome-specific fairness auditing. Full results are presented in Appendix B.

\subsection{Bias Mitigation Results}

We applied threshold optimization to equalize TPR across groups, targeting the overall model TPR of 0.283. Table \ref{tab:mitigation} presents the results.

\begin{table*}[ht!]
\centering
\caption{Bias Mitigation Results (Threshold Optimization)}
\label{tab:mitigation}
\begin{tabular}{lcccccc}
\toprule
\textbf{Group} & \textbf{TPR Bef.} & \textbf{TPR Aft.} & \textbf{$\Delta$TPR} & \textbf{Acc.\ Bef.} & \textbf{Acc.\ Aft.} & \textbf{$\Delta$Acc.} \\
\midrule
White & 0.160 & 0.293 & +0.133 & 0.891 & 0.886 & $-$0.005 \\
Black & 0.296 & 0.293 & $-$0.003 & 0.747 & 0.747 & +0.000 \\
Hispanic & 0.393 & 0.295 & $-$0.098 & 0.775 & 0.762 & $-$0.013 \\
Asian & 0.760 & 0.250 & $-$0.510 & 0.875 & 0.625 & $-$0.250 \\
Other & 0.258 & 0.296 & +0.038 & 0.902 & 0.884 & $-$0.018 \\
\bottomrule
\end{tabular}
\end{table*}

Threshold optimization successfully equalized TPR across the major demographic groups (White, Black, Hispanic, Other all achieving TPR $\approx$ 0.29). However, this equalization came at a cost that raises ethical concerns. Hispanic students experienced reduced sensitivity ($-$0.098), meaning the ``fair'' system would identify \textit{fewer} at-risk Hispanic students to achieve parity with the White detection rate. Asian students experienced a catastrophic drop in both TPR ($-$0.510) and accuracy ($-$0.250), though the small sample size ($N = 8$) renders this estimate unreliable. The group-specific thresholds ranged from 0.367 (White) to 0.649 (Asian), indicating that predictions for different groups required substantially different decision boundaries to achieve equitable outcomes.

These results illustrate two fundamental problems with naive post-hoc equalization. First, equalizing to a low overall TPR (0.283) can \textit{harm the very groups it intends to help}: Hispanic students, who were being identified at a higher rate under the uncorrected model, would receive fewer interventions after equalization. This is a direct consequence of the impossibility results of \citet{chouldechova2017fair}: when base rates differ across groups, no classifier can simultaneously equalize TPR, FPR, and PPV. Second, implementing group-specific thresholds requires the system to ``know'' each student's race at decision time, creating a race-conscious classification mechanism that faces both legal scrutiny and practical concerns about reinforcing racial categorization. In-processing alternatives---such as adversarial debiasing or fairness constraints during training---may offer more defensible approaches by addressing disparity at its source rather than through post-hoc adjustment.

%==============================================================================
\section{Discussion}
%==============================================================================

\subsection{Summary of Findings}

This study conducted a multi-dimensional fairness audit of machine learning models that use early childhood data to predict 5th-grade academic risk. Our key findings include:

\begin{enumerate}
    \item \textbf{Classical models match state-of-the-art methods:} Among seven algorithms tested---including three 2025 gradient boosting variants---classical regularized models (Elastic Net: AUC = 0.848; Logistic Regression: AUC = 0.847) matched or exceeded LightGBM (0.837), CatBoost (0.846), and HistGradientBoosting (0.839). This is consistent with recent findings that tree-based methods do not universally dominate on tabular data \citep{grinsztajn2022tree} and suggests that algorithmic sophistication does not substitute for equitable design.

    \item \textbf{Statistically significant fairness disparities:} Bootstrap confidence intervals confirmed that the Hispanic-White TPR gap (0.233) is statistically significant, with non-overlapping 95\% CIs. The model detected at-risk Hispanic students at 2.5 times the rate of White students, while generating 8.5 times higher false positives for Black students.

    \item \textbf{Severe calibration unfairness:} Black students experienced calibration error 3.35 times higher than White students (ECE = 0.074 vs. 0.022). This means that when the model predicts a Black student has, say, a 40\% probability of being at-risk, the actual probability may be substantially different. Calibration unfairness represents a distinct dimension of algorithmic harm beyond classification error rates.

    \item \textbf{Intersectional patterns consistent with poverty detection:} The model identified no at-risk Black students in the 4th SES quintile (TPR = 0\%, $N = 42$, $\approx$6 at-risk), though this cell size precludes definitive conclusions ($p = 0.41$). This suggestive pattern of ``intersectional invisibility,'' combined with the strong SES gradient visible across all groups, is consistent with a model that operates primarily through socioeconomic signals---a hypothesis that warrants replication with larger intersectional samples.

    \item \textbf{Temporal fairness paradox:} Additional longitudinal data improved accuracy (AUC 0.799 to 0.831) but failed to resolve---and in some cases worsened---fairness disparities. This \textit{temporal fairness paradox} challenges the assumption that more data naturally produces fairer predictions.

    \item \textbf{Threshold fragility:} Fairness compliance was achieved at only one of four tested thresholds (25th percentile), revealing that claims of fairness are contingent on specific analytical choices.

    \item \textbf{Mitigation raises ethical red flags:} Threshold optimization achieved equitable TPR across groups but \textit{reduced} identification of at-risk Hispanic students to match the lower White detection rate. Naive post-hoc equalization can harm the very groups it intends to help, and group-specific thresholds constitute race-conscious classification with legal implications.

    \item \textbf{Missing data strengthens conclusions:} Multiple imputation on the full sample ($N = 18{,}151$) yielded higher TPR disparities (Black MICE TPR = 0.530 vs.\ complete-case 0.293; Hispanic 0.552 vs.\ 0.410) and similar AUC (0.864 vs.\ 0.848), confirming that complete-case estimates are conservative. Inverse probability weighting produced nearly identical results to unweighted analysis (AUC = 0.848).
\end{enumerate}

The convergence of seven algorithms on nearly identical performance (AUC range of 0.011) shifts the locus of the fairness problem from the model to the data. When classical logistic regression, elastic net, random forest, XGBoost, LightGBM, CatBoost, and HistGradientBoosting all produce equivalent predictions with equivalent fairness failures, the implication is clear: the source of inequity is structural, embedded in the features and outcome definitions, not in any particular algorithmic choice. This finding argues against the common hope that switching to a more sophisticated algorithm will resolve fairness concerns.

\subsection{Interpreting Fairness Disparities}

The observed fairness disparities require careful interpretation. Several factors may contribute:

\textbf{Differential base rates:} At-risk prevalence was substantially higher among Black (25.0\%) and Hispanic (29.4\%) students compared to White students (11.9\%). When base rates differ, even a well-calibrated model will exhibit different error rates across groups \citep{chouldechova2017fair}.

\textbf{Proxy discrimination:} Although race was excluded from the model, other features (particularly SES) are correlated with race and may serve as proxies. The elastic net assigned substantial weight to SES (mean $|$SHAP$|$ = 0.253), which could contribute to differential performance.

\textbf{Structural inequities:} The patterns in the data reflect historical and ongoing structural inequities in educational opportunity. Children from disadvantaged backgrounds may show weaker early signals not because of inherent ability, but because of differential access to high-quality early childhood education.

\textbf{The poverty detector problem:} The intersectional analysis suggests the model may function primarily as a poverty detector, though small cell sizes require cautious interpretation. SES is the third-strongest predictor, and its effects are deeply entangled with cognitive scores---which themselves reflect socioeconomic advantage. This creates a pattern where the model identifies low-SES children of all races but may miss at-risk children who come from relatively advantaged backgrounds. The MICE analysis on the full sample strengthens this interpretation: when attrition-related selection bias is addressed, the SES-driven disparities become \textit{more} pronounced, not less, suggesting that complete-case analysis understates the degree to which socioeconomic signals dominate the model's behavior.

\textbf{Calibration and trust:} Calibration unfairness is particularly consequential because practitioners rely on predicted probabilities, not just binary classifications, to prioritize interventions. If a school counselor sees that two students both have a 30\% predicted risk, they may allocate resources equally---but if the model is poorly calibrated for Black students, these probability estimates carry unequal information content \citep{pleiss2017calibration}.

\subsection{Implications for Educational Practice}

Our findings have important implications for the deployment of predictive analytics in K-12 education:

\textbf{Multi-dimensional fairness audits are essential:} Standard single-metric fairness assessments are insufficient. Our model passed equal opportunity while failing equalized odds, showed severe calibration disparities, and exhibited intersectional blind spots. A comprehensive audit examining group fairness, calibration, and intersectional subgroups is necessary before deployment.

\textbf{Intersectional auditing is necessary:} Standard fairness audits examining single protected attributes miss compounding disadvantages. Our finding that high-SES Black students are completely invisible to the model would not have been detected by a race-only or SES-only analysis \citep{buolamwini2018gender, kearns2018preventing}.

\textbf{Context and threshold choice matter:} The sensitivity of fairness to threshold choice means that the definition of ``at-risk'' is not merely a technical parameter---it is a policy decision with fairness implications that should involve stakeholder input. Different thresholds serve different populations, and no single threshold produces equitable outcomes across all metrics.

\textbf{Temporal deployment trade-offs:} Our temporal analysis suggests that the optimal prediction window involves a trade-off. Earlier predictions (kindergarten only) enable earlier intervention but with lower accuracy. Later predictions (through 3rd grade) improve accuracy but narrow the intervention window and may actually worsen fairness gaps.

\textbf{Human oversight remains critical:} Predictive models should inform, not replace, human judgment. Educators and counselors should understand model limitations and exercise discretion in interpreting predictions, particularly for demographic subgroups where the model is poorly calibrated.

\subsection{Generalizability to Post-Pandemic Contexts}

The ECLS-K:2011 cohort (2010--2016) preceded the COVID-19 pandemic, which produced learning losses that were both substantial and sharply stratified by race and socioeconomic status \citep{fahle2023learning}. Evidence from standardized assessments indicates that achievement gaps widened considerably during 2020--2022, with low-income students and students of color experiencing the steepest declines. This has two implications for the generalizability of our findings. First, the SES-achievement gradient that underlies the ``poverty detector'' pattern likely \textit{intensified} post-pandemic, suggesting that the fairness disparities we document represent a \textit{floor} rather than a ceiling for contemporary early warning systems. Second, models trained on pre-pandemic data would face distributional shift when applied to post-pandemic cohorts, as the statistical relationships between early childhood predictors and later outcomes have been disrupted. Future fairness audits should explicitly test cross-cohort generalization across the pre-/post-pandemic boundary, as the ECLS-K:2011 findings may understate the severity of fairness failures in current educational contexts.

\subsection{Limitations}

This study has several limitations:

\begin{itemize}
    \item \textbf{Public-use data constraints:} The public-use ECLS-K:2011 file has some variables suppressed or top-coded to protect confidentiality, potentially limiting predictive power.

    \item \textbf{Missing data and attrition:} Our primary analysis used complete cases ($N = 9{,}104$), representing 50\% of the original cohort. Attrition analysis revealed that dropouts had significantly lower baseline cognitive scores (Cohen's $d \geq 0.20$ on all cognitive variables) and were disproportionately from minority and lower-SES backgrounds (e.g., White representation: 53.4\% among completers vs.\ 40.0\% among dropouts). MICE sensitivity analysis on the full sample ($N = 18{,}151$) produced \textit{larger} fairness disparities (Black TPR increased from 0.293 to 0.530; Hispanic from 0.410 to 0.552), confirming that complete-case estimates are conservative. IPW reweighting produced virtually identical results (AUC = 0.848), with well-behaved weights (mean = 0.77, SD = 0.08, range 0.58--1.03). While these sensitivity analyses support the robustness of our primary conclusions, the analytic sample likely under-represents the most disadvantaged students, and the true magnitude of fairness disparities may be larger than reported.

    \item \textbf{Single pre-pandemic cohort:} The ECLS-K:2011 followed a single cohort (2010--2016) that preceded the COVID-19 pandemic. Post-pandemic learning losses were substantial and sharply stratified by race and SES \citep{fahle2023learning}, suggesting that the SES-achievement gradient underlying our findings has likely steepened. Models trained on pre-pandemic data would face distributional shift when applied to contemporary cohorts. Future audits should test cross-cohort generalization across the pre-/post-pandemic boundary.

    \item \textbf{Binary outcome:} We operationalized risk as a binary threshold ($<$25th percentile). Alternative operationalizations---as demonstrated by our sensitivity analysis---yield different fairness results.\looseness=-1

    \item \textbf{Small subgroup sizes:} The Asian subgroup ($N = 8$ in the test set) is too small for any reliable inference and should not inform policy conclusions. Intersectional subgroups with 3--6 positive cases (e.g., Black Q4 with $\approx$6 at-risk students) fall below thresholds for reliable estimation; the observed TPR = 0\% is not statistically distinguishable from the overall model TPR ($p = 0.41$). These patterns are suggestive but require replication with larger, purpose-sampled datasets before drawing definitive conclusions.

    \item \textbf{Post-hoc mitigation only:} We examined only post-hoc threshold adjustment. In-processing methods (e.g., adversarial debiasing, fairness constraints during training) might achieve better accuracy-fairness trade-offs.

    \item \textbf{Temporal design:} Our temporal analysis held the outcome constant (5th grade) while varying inputs. A complementary approach varying both inputs and outcomes would provide additional insight.
\end{itemize}

\subsection{Future Directions}

Several directions for future research emerge from this study:

\begin{itemize}
    \item \textbf{In-processing fairness methods:} Future work should evaluate constraint-based methods that incorporate fairness during model training, potentially achieving better accuracy-fairness trade-offs than post-hoc threshold adjustment.

    \item \textbf{Causal fairness methods:} Approaches that distinguish between legitimate and illegitimate predictive pathways could help identify which features contribute to fairness disparities through discriminatory versus non-discriminatory mechanisms.

    \item \textbf{Multi-objective optimization:} Jointly maximizing accuracy and minimizing group fairness disparities during training represents a promising approach to balancing competing objectives.

    \item \textbf{Restricted-use data:} Replication with restricted-use ECLS data, which contains additional variables suppressed in the public-use file, could improve both predictive power and fairness.

    \item \textbf{Intervention studies:} Ultimately, the value of EWS depends on whether they improve outcomes. Randomized studies examining the causal effect of EWS-informed interventions, with attention to differential effects across groups, are needed.
\end{itemize}

%==============================================================================
\section{Conclusion}
%==============================================================================

This study provides evidence that fairness failures in educational risk prediction are properties of the prediction task itself, not artifacts of model architecture. Seven algorithms---including three state-of-the-art gradient boosting methods---converged on nearly identical performance (AUC range = 0.011) and exhibited the same pattern of fairness disparities. When every algorithm fails in the same way, the source of inequity is structural: embedded in the features, outcome definitions, and social stratification that shapes the training data.

Our multi-dimensional fairness audit---spanning group-level metrics with bootstrap confidence intervals, calibration error analysis, intersectional subgroup assessment, temporal generalization, sensitivity analysis, and missing data robustness checks---reveals that no single fairness assessment captures the full picture. The model passes equal opportunity at the 25th percentile but fails at all other thresholds. It fails calibration fairness, with Black students experiencing 3.35 times higher calibration error. Intersectional analysis suggests it may function as a poverty detector, though small cell sizes require cautious interpretation ($N = 42$ for Black Q4, $p = 0.41$). The \textit{temporal fairness paradox}---that additional data improves accuracy without resolving fairness---challenges a central assumption of longitudinal prediction systems.

These findings argue for comprehensive, multi-dimensional fairness auditing as a prerequisite for deploying algorithmic systems in education. The convergence of seven algorithms on equivalent fairness failures shifts the policy conversation from algorithmic choice to data equity and institutional reform. As predictive analytics become increasingly prevalent in K-12 settings, researchers and practitioners must adopt the tools and frameworks demonstrated here---SHAP explainability, bootstrap uncertainty quantification, calibration analysis, and intersectional auditing---to ensure these systems serve all students equitably.

%==============================================================================
% Ethics Statement
%==============================================================================

\section*{Ethics Statement}

This study uses the Early Childhood Longitudinal Study, Kindergarten Class of 2010--11 (ECLS-K:2011) public-use data file, which is freely available from the National Center for Education Statistics (NCES) without restricted-use license or institutional review board (IRB) approval. All data are fully de-identified by NCES prior to public release; no individual students, families, or schools can be identified. Because this research involves secondary analysis of existing de-identified public-use data, it is exempt from IRB review under 45 CFR 46.104(d)(4). No participants were contacted or recruited for this study.

We acknowledge that predictive models for educational risk carry ethical implications beyond technical performance. Our fairness audit is intended to inform responsible development and deployment practices---not to endorse the use of any particular model in high-stakes educational decision-making without further validation, stakeholder engagement, and ongoing monitoring.

%==============================================================================
% Data and Code Availability
%==============================================================================

\section*{Data and Code Availability}

The ECLS-K:2011 public-use data file is freely available from the National Center for Education Statistics at \url{https://nces.ed.gov/ecls/dataproducts.asp}. All analysis code, configuration files, and scripts to reproduce the results reported in this paper are available at [GitHub repository URL]. A permanent archival copy of the code is deposited at [Zenodo DOI]. The repository includes a complete pipeline (\texttt{scripts/run\_pipeline.py}) that reproduces all results, figures, and tables from the raw ECLS data file.

%==============================================================================
% Acknowledgments
%==============================================================================

\section*{Acknowledgments}

[Removed for double-blind review.]

%==============================================================================
% Conflict of Interest
%==============================================================================

\section*{Conflict of Interest}

The authors declare no competing interests.

%==============================================================================
% AI Disclosure
%==============================================================================

\section*{Use of Generative AI}

Generative AI (Claude, Anthropic) was used as a coding assistant during data analysis pipeline development, figure generation, and manuscript formatting. All scientific content---including research design, interpretation of results, and substantive writing---was produced by the authors. The authors reviewed and verified all AI-assisted outputs for accuracy. No AI tool was used to generate or alter the study's data, statistical analyses, or scholarly conclusions.

%==============================================================================
% References
%==============================================================================

\newpage
\bibliographystyle{SageH}
\bibliography{references}

%==============================================================================
% Appendix
%==============================================================================

\newpage
\appendix
\section{Technical Details}

\subsection{Software and Reproducibility}

All analyses were conducted in Python 3.12 using the following packages:
\begin{itemize}
    \item scikit-learn $\geq$ 1.4.0 (including HistGradientBoostingClassifier)
    \item xgboost $\geq$ 2.0.0
    \item lightgbm $\geq$ 4.3.0
    \item catboost $\geq$ 1.2.0
    \item shap $\geq$ 0.45.0
    \item fairlearn $\geq$ 0.10.0
    \item pandas, numpy, matplotlib, seaborn
\end{itemize}

Random seed was set to 42 for all stochastic operations. Code and data processing scripts are available in the project repository.

\subsection{Model Hyperparameters}

The final elastic net model used the following hyperparameters selected via 5-fold cross-validation:
\begin{itemize}
    \item Regularization strength ($\alpha$): 0.01
    \item L1 ratio: 0.5
    \item Maximum iterations: 1000
\end{itemize}

Cross-validation AUC scores ranged from 0.832 to 0.842 across folds, indicating stable performance.

\subsection{Missing Data}

Table \ref{tab:missing} presents missing data rates for key variables. See Section 3.7 for the full missing data sensitivity analysis methodology and Section 4.6 for results.

\begin{table*}[ht!]
\centering
\caption{Missing Data Rates}
\label{tab:missing}
\begin{tabular}{lcc}
\toprule
\textbf{Variable} & \textbf{N Missing} & \textbf{\% Missing} \\
\midrule
5th Grade Reading (Outcome) & 6,724 & 37.0\% \\
Executive Function (X6DCCSSCR) & 4,379 & 24.1\% \\
1st Grade Approaches to Learning & 4,708 & 25.9\% \\
Fall K Reading & 2,482 & 13.7\% \\
SES Quintile & 2,063 & 11.4\% \\
Home Language & 2,106 & 11.6\% \\
Spring K Reading & 965 & 5.3\% \\
\bottomrule
\end{tabular}
\end{table*}

The high rate of missing outcome data (37\%) reflects sample attrition over the longitudinal study. As documented in Section 4.6, completers differed systematically from dropouts on all baseline cognitive variables (Cohen's $d = 0.22$--$0.30$), with White students over-represented among completers. MICE and IPW sensitivity analyses confirmed that complete-case fairness estimates are conservative.

\newpage
\section{Supplementary Results}

All supplementary figures, tables, and the full math outcome comparison are provided in the Online Supplementary Materials document, which includes: PPV by group with confidence intervals (Figure S1); SHAP vs.\ permutation importance comparison (Figure S2, Table S1); SHAP importance by racial/ethnic group (Figure S3); temporal fairness trends (Figures S4--S6); permutation importance with bootstrap CIs (Table S2); calibration error across temporal scenarios (Table S3); detailed sensitivity analysis (Table S4); temporal fairness group-level metrics (Table S5); and reading vs.\ math outcome comparison (Tables S6--S8).

\end{document}
